% FAQ: Entropy Has No Direction â€” live Q&A
% Prepared by Ting Peng (t.peng@ieee.org)
%
% About:
% @misc{peng2026entropydirectionmirrorstateparadox,
%       title={Entropy Has No Direction: A Mirror-State Paradox Against Universal Monotonic Entropy Increase and a First-Principles Proof that Constraints Reshape the Entropy Distribution $P_{\infty}(S;\lambda)$},
%       author={Ting Peng},
%       year={2026},
%       eprint={2602.15369},
%       archivePrefix={arXiv},
%       primaryClass={cond-mat.stat-mech},
%       url={https://arxiv.org/abs/2602.15369},
% }

\documentclass[11pt,a4paper]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\usepackage{hyperref}
\emergencystretch=1em

\title{Frequently Asked Questions\\
\large Entropy Has No Direction: Mirror-State Paradox and Constraint-Reshaped Distributions}
\author{Ting Peng\\
  \texttt{t.peng@ieee.org}\\
  Key Laboratory for Special Area Highway Engineering of Ministry of Education,\\
  Chang'an University, Xi'an 710064, China}
\date{\today}

\begin{document}
\maketitle

\noindent
This FAQ accompanies the paper:

\medskip
\noindent
T.~Peng, \emph{Entropy Has No Direction: A Mirror-State Paradox Against Universal Monotonic Entropy Increase and a First-Principles Proof that Constraints Reshape the Entropy Distribution $P_{\infty}(S;\lambda)$}, arXiv:2602.15369 [cond-mat.stat-mech] (2026).\\
\url{https://arxiv.org/abs/2602.15369}

\medskip
\noindent
The \LaTeX\ source of the manuscript and this FAQ are available at \url{https://github.com/tpeng1977/entropy}. For the full argument, proofs, and references, see the manuscript \texttt{entropy.tex} / \texttt{entropy.pdf}.

\begin{enumerate}[leftmargin=*]

\item \textbf{What is the mirror-state paradox?}

For any microstate $A$ (phase point $\Gamma_A$) at time $t_0$, define the \emph{mirror state} $B$ by $\Gamma_B(t_0)=\mathcal{T}\Gamma_A(t_0)$ (time reversal: same positions, reversed momenta). Under time-reversal invariant dynamics, the forward-time evolution of $B$ replays the past of $A$ in reverse. If a \emph{universal} law said ``entropy does not decrease'' for every microstate and every time, then applied to both $A$ and $B$ it would imply $S_A(t_0+\delta t) \ge S_A(t_0)$ and $S_A(t_0-\delta t) \ge S_A(t_0)$ for small $\delta t > 0$. So $t_0$ is a two-sided local minimum of $S_A(t)$. Since $t_0$ is arbitrary, every time is a local minimum; with minimal regularity (e.g.\ continuity of $S$ along trajectories), entropy must be constant on every trajectory. So a universal monotonicity claim is logically incompatible with time-reversal symmetry and would remove any entropic arrow of time.

\item \textbf{Does this disprove the Second Law of Thermodynamics?}

It shows that the Second Law \emph{cannot} be a \emph{universal} fundamental law in the form ``entropy does not decrease'' (either strict trajectory-wise or as a universal statistical principle) when microscopic dynamics are time-reversal invariant. The paper does not deny that in many practical settings entropy tends to increase; it denies that this follows as a universal, state-by-state consequence of the underlying physics. Any one-way statement about $\Delta S$ must rely on additional structure: special initial ensembles, coarse-graining choices, limits, or constraints/boundaries (as stated in the paper).

\item \textbf{What replaces ``entropy has a direction''?}

The consistent view: \textbf{entropy has no direction; it is a stochastic variable described by a probability distribution} $P(S)$ (paper abstract and Corrected view). The shape of this distribution depends on constraints and boundary conditions (encoded as $\lambda$). Long-time behavior is captured by $P_{\infty}(S;\lambda)$, the entropy distribution induced by the invariant measure under those constraints.

\item \textbf{What is $P_{\infty}(S;\lambda)$?}

$P_{\infty}(S;\lambda)$ is the \emph{long-time} entropy distribution induced by the invariant measure: the probability distribution of (coarse-grained) entropy $S$ in the limit of long times, under constraint parameters $\lambda$ (geometry, boundary conditions, static fields, etc.). It is made precise either by ergodic/mixing dynamics or by an initial ensemble drawn from the invariant measure (microcanonical or canonical). The paper proves from first principles that changing $\lambda$ (Hamiltonian and/or accessible phase space) changes this distribution.

\item \textbf{How do constraints reshape the entropy distribution?}

Constraints enter via the Hamiltonian $H(\Gamma;\lambda) = H_0(\Gamma) + V_{\mathrm{c}}(\Gamma;\lambda)$ and/or the accessible set $\mathcal{A}(\lambda)$. They change the invariant measure (e.g.\ uniform on the energy shell in the microcanonical case) and hence the macrostate probabilities $\pi_m^{(E)}(\lambda) = W_m^{(E)}(\lambda)/\Omega(E;\lambda)$. So the induced distribution over entropy values $P_{\infty}^{(E)}(S;\lambda)$ changes when the accessible macrostate volumes $W_m^{(E)}(\lambda)$ change with $\lambda$.

\item \textbf{When does $P_{\infty}^{(E)}(S;\lambda)$ stay the same (up to translation)?}

Only in one case: when the multiset of accessible macrostate volumes $\{W_m^{(E)}(\lambda)\}$ is scaled by a common factor $c > 0$ (up to permutation). Then $P_{\infty}^{(E)}(S;\lambda_2)$ is just $P_{\infty}^{(E)}(S - k_B\ln c;\lambda_1)$. Otherwise the distribution changes \emph{structurally}, not merely by shifting the entropy axis.

\item \textbf{What does the Qiao--Wang experiment show?}

Qiao and Wang showed that in charged small nanopores (effective pore size $d_e \approx 1\,\mathrm{nm}$, ion size $d_i \approx 0.7\,\mathrm{nm}$, so $d_i < d_e < 2d_i$) the steady-state ion distribution is intrinsically out of equilibrium: the potential difference can be nearly an order of magnitude larger than the heat-engine upper bound from the traditional Second Law. The system can produce useful work in an isothermal cycle by absorbing heat from a single thermal reservoir. This is interpreted as the asymmetric constraint (nanopore geometry) reshaping $P_{\infty}(S;\lambda)$, making spontaneous low-entropy transitions accessible without requiring compensating entropy increase elsewhere. The paper (Sec.~Experimental validation) describes the setup as nanoporous carbon electrodes in dilute aqueous cesium pivalate (CsPiv) solutions, with quasi-one-dimensional ion lineups when $d_i < d_e < 2d_i$; the constraint reshapes $\mathcal{A}(\lambda)$ and hence $P_{\infty}(S;\lambda)$.

\item \textbf{Does this mean a perpetual motion machine of the second kind is possible?}

In the corrected framework, the traditional ``impossibility'' intuition behind a perpetual motion machine of the second kind is no longer a fundamental axiom but a limit-dependent, model-dependent statement; in that precise sense, the road toward a perpetual motion machine of the second kind is now faintly visible. The experimental validation by Qiao and Wang demonstrates that this road is not merely visible but has been traversed: asymmetric constraints can reshape the entropy distribution to enable spontaneous low-entropy transitions, allowing a system to produce useful work in a cycle by absorbing heat from a single thermal reservoir. With this experimental confirmation, both the theoretical and experimental barriers to a perpetual motion machine of the second kind have been overcome. The path forward is through iterative enhancement, engineering optimization, and scaling of constraint-based entropy reshaping technologies; practical devices can be developed.

\item \textbf{What about heat death?}

If entropy does not have a universal direction and constraints can reshape $P_{\infty}(S;\lambda)$, then a \emph{universal} heat death of the universe is not a necessary consequence of the same first principles. The paper does not make detailed cosmological claims but notes implications for long-term sustainability and the possibility of sustained non-equilibrium phenomena under suitable constraints.

\item \textbf{Which definition of entropy is used?}

Boltzmann (coarse-grained) entropy: a time-reversal symmetric partition of phase space into macrostates $\{C_m\}$, with $S_m^{(E)} = k_B\ln(W_m^{(E)}/W_0)$ on the energy shell. The conclusions depend on this choice only in that the coarse-graining is assumed time-reversal symmetric, $S(\mathcal{T}\Gamma)=S(\Gamma)$.

\item \textbf{A critic says the paper conflates ``time-reversed trajectory'' with ``physically attainable future'' and commits a classic error. How do you respond?}

The paper does not treat the time-reversed trajectory as a ``future the system will actually follow.'' The argument only does the following: if one asserts a \emph{universal} law (``entropy does not decrease for every microstate and every time''), then that statement must also hold when applied to the mirror state $B$ at $t_0$. The \emph{forward} evolution of $B$ is, mathematically, $A$'s past. So one obtains $S_A(t_0-\delta t)\ge S_A(t_0)$ and $S_A(t_0+\delta t)\ge S_A(t_0)$, i.e.\ entropy is a two-sided local minimum at every $t_0$, hence constant along the trajectory---contradicting macroscopic entropy increase. So a \emph{universal} entropy-nondecrease claim is logically incompatible with time-reversal symmetry and must be dropped or qualified. The paper concludes that any one-way statement about $\Delta S$ must depend on additional structure (initial ensemble, coarse-graining, constraints). That is consistent with the view that the arrow of time comes from initial conditions, not from a universal dynamical law.

\item \textbf{Doesn't ``constraint'' just mean a different box, with entropy still maximizing inside the box?}

The paper agrees that for a \emph{given} constraint $\lambda$, the long-time behavior is given by the invariant measure under that $\lambda$, i.e.\ the system tends to ``equilibrium'' in that accessible subspace. The claim is that \emph{different} constraints $\lambda$ yield \emph{different} long-time entropy distributions $P_{\infty}(S;\lambda)$; when $\lambda$ changes, the \emph{shape} of the distribution changes (unless all macrostate volumes are scaled by a common factor). So ``changing the box'' is exactly what the paper asserts: different $\lambda$ means different $P_{\infty}$. Under strong or asymmetric constraints, the long-time distribution can be concentrated in a relatively low entropy range---i.e.\ the system's entropy is more likely to be distributed in that lower range. There is no notion of entropy ``reaching'' or ``attaining'' some value; entropy is dynamically changing and does not settle at a particular value. The paper does not claim that inside one and the same box entropy fails to maximize; it claims that different boxes give different distributions.

\item \textbf{What is the status of entropy in physics---is it a ``law'' or something else?}

Entropy is not a physical law on a par with the equations of motion; it is a \emph{statistical quantity} that arises when the system evolves under those laws. The constituents follow the dynamical laws (e.g.\ Hamiltonian dynamics); entropy is a scalar function $S(\Gamma)$ of the trajectory under some coarse-graining, and its time dependence $S(t)=S(\Gamma(t))$ is entirely determined by the dynamics and the initial state. So the \emph{laws} are the equations of motion; how entropy changes is determined by those laws, not by a separate ``entropy-increase law'' that the system obeys. The traditional ``entropy monotonically increases'' formulation is an \emph{empirical and statistical summary} under the condition that systems start from low entropy and under specific constraints---not a universal scientific law.

\item \textbf{How do you summarize in one sentence the ``corrected view'' versus the ``traditional entropy-increase view''?}

The traditional view treats ``entropy does not decrease'' as a universal law that holds for every state and every time; the corrected view is that \textbf{entropy has no intrinsic direction and is described by a probability distribution} $P(S)$ (and $P_{\infty}(S;\lambda)$ under constraints), and any one-way entropy behavior depends on additional structure such as initial ensemble, coarse-graining, and constraints. So: entropy increase is not the law; the equations of motion are the law. Entropy is a statistical quantity produced as the system runs; its behavior is determined by the dynamics and the conditions, not by an independent scientific law.

\item \textbf{Can entropy as a single number adequately describe the system and time reversal?}

No. Entropy is a mapping of a high-dimensional, complex system onto a one-dimensional axis and therefore discards a great deal of information; it only captures ``surface order'' in a coarse-grained sense and cannot fully describe the system state. Time reversal is defined on the \emph{full microscopic state} $\Gamma$ (e.g.\ positions and momenta), e.g.\ $\mathcal{T}\Gamma$ (reverse momenta). The mirror state is $\Gamma_B = \mathcal{T}\Gamma_A$---an operation on the phase point, not on the number $S$. The paper assumes time-reversal symmetric coarse-graining so that $S(\mathcal{T}\Gamma)=S(\Gamma)$, and uses entropy only to compare two trajectories (the future of $A$ and the future of $B$, which is $A$'s past). So an exact description of time reversal cannot rely on entropy alone but on the full state. In simulation one can implement ideal reversal exactly; in the real world, preparing a state that is exactly the time reverse of another is extremely difficult. This underscores both the coarse-grained nature of entropy and the practical difficulty of ``full'' reversal.

\item \textbf{Critics say nanopore-style experiments cannot support ``sustained net work'' or a second-kind perpetual motion machine. What is the paper's position?}

The paper cites the Qiao--Wang experiment within the framework of ``constraint-reshaped long-time entropy distribution'' as an example of a non-equilibrium steady state and of a distribution concentrated at lower entropy under constraint, and states that theoretical and experimental barriers have been overcome. If critics insist that boundaries, environment coupling, measurement, and entropy production under sustained energy extraction must be fully accounted for and conclude that sustained net work is impossible or still Carnot-limited, that is a dispute about \emph{experimental interpretation and thermodynamic accounting}, to be settled at the level of experiments and heat-engine bounds. It does not amount to a refutation of the paper's mirror-state \emph{logic} or the mathematical derivation of $P_{\infty}(S;\lambda)$ from statistical mechanics; the former is about application and interpretation, the latter about the incompatibility of a universal entropy law with time-reversal symmetry and the constraint-dependence of the distribution.

\item \textbf{What is entropy in essence, and what actually determines how the system evolves?}

Entropy is only a \emph{statistical quantity} that expresses the degree of disorder of the system; it is \emph{not} what determines the system's evolution. As long as the system keeps evolving under its dynamics, entropy will keep changing---sometimes increasing, sometimes decreasing---spontaneously, because the dynamics are still running. The essence of entropy is that it is a \emph{mapping} of a high-dimensional, complex system in motion onto a one-dimensional axis; it is a reduced description, not the underlying cause. Its practical meaning is that by \emph{prearranged intervention} (e.g.\ constraints and boundary conditions) we can steer the system so that the long-time distribution of entropy is concentrated in ranges we want, and in that way put the system to use. The paper's ``constraints reshape the probability distribution of entropy'' is exactly this: we change the conditions $\lambda$ and the long-time entropy distribution $P_{\infty}(S;\lambda)$ changes accordingly. But what \emph{actually} drives the system is the \emph{laws of motion}; entropy is only a surface manifestation of that dynamics. The laws of motion are the essence; entropy is the appearance.

\item \textbf{Why do constraints reshape the entropy distribution? What is the underlying mechanism?}

In essence, constraints \emph{affect how the particles in the system move}; that change in the motion then affects the \emph{statistics} of how the system runs---and entropy is precisely that statistical quantity. So ``constraints reshape the entropy distribution'' means: by changing $\lambda$ (geometry, boundaries, potentials), we alter the trajectories and the accessible phase space; the long-time distribution of the coarse-grained state (and hence of entropy) $P_{\infty}(S;\lambda)$ changes because the underlying motion has changed. As long as the dynamics keep running, entropy---being a statistical quantity that summarizes the motion---keeps changing; the distribution we observe or compute reflects that ongoing evolution under the given constraints.

\item \textbf{Are phenomena ``right'' or ``wrong''? Where does the traditional Second Law go wrong?}

Phenomena are not right or wrong; they are objectively there. That entropy often increases in many practical settings is an observed fact. The error is \emph{taking} what is observed repeatedly under certain conditions and \emph{fitting} it as a universal law valid for the whole universe---i.e.\ asserting that ``entropy does not decrease'' holds for every microstate and every time as a fundamental dynamical law. So: the phenomenon (frequent entropy increase in some regimes) is real; the mistake is elevating that pattern to a universal law.

\item \textbf{Is it methodologically sound to study ``laws of entropy''?}

No. Entropy is essentially a \emph{projection} of a high-dimensional, complex state onto a low-dimensional quantity; studying ``laws of entropy'' means studying the behavior of that projection, not the underlying dynamics. Methodologically, the right object of study is the \emph{laws of motion} (e.g.\ Hamiltonian dynamics); entropy is a derived, statistical quantity that summarizes how the system runs under those laws. One should study the motion first; entropy then appears as an outcome, not as an independent subject of ``entropy laws.''

\item \textbf{For energy harvesting or matter separation, do we need to compute entropy to decide if it's feasible?}

No. Entropy is only a statistical quantity (a one-dimensional projection of high-dimensional state); it is not the right criterion for deciding whether we can ``get something out'' of the system. The real question is whether \emph{thermal motion is converted into electricity} (or work) or whether \emph{matter is effectively separated} under the given constraints. That is decided by actual energy and mass flows, conversion efficiency, and dynamics---not by computing entropy. So: decide based on whether the conversion happens and whether there is a gain; entropy does not need to be the basis for that decision.

\item \textbf{How can we think of entropy in terms of ``gathering'' and ``opportunity''?}

One can think of entropy as a statistic that reflects whether ``the fish have gathered'': when the system is more concentrated or ordered (in a coarse-grained sense), entropy tends to be lower---``fish gathered,'' opportunity to harvest; when it is dispersed, entropy is higher---``fish scattered,'' little to exploit. So entropy \emph{fluctuates} (fish gather and disperse); it does not settle at a fixed value. We can use constraints and design (the ``methods'') to make the system spend more time in favorable states: just as we can create conditions for fish to gather, we can choose $\lambda$ so that the long-time distribution $P_{\infty}(S;\lambda)$ has \emph{greater weight in the lower-entropy range}. Then the system is more often in a state where we can ``scoop''---harvest energy or separate matter---without entropy being the criterion for the decision; the criterion remains whether we actually get the conversion and the gain.

\item \textbf{What does ``constraints give $P_{\infty}(S;\lambda)$ more weight at lower entropy'' mean in practice?}

It means that under the chosen constraint parameters $\lambda$ (geometry, boundaries, potentials), the long-time probability distribution of entropy is shifted or shaped so that \emph{lower entropy values are more probable} than they would be under a different $\lambda$. The system's entropy still fluctuates over time; it does not ``reach'' and stay at one value. But on average, over long times, it is more often in a lower-entropy range---i.e.\ more often in a state where extraction (energy harvesting, separation) is possible. So we ``make the fish gather'' more often by choosing $\lambda$ so that $P_{\infty}(S;\lambda)$ has greater weight in the lower-entropy region.

\item \textbf{Why can't we learn the underlying dynamics by studying entropy?}

Entropy is essentially a \emph{projection} of high-dimensional state onto a low-dimensional quantity; studying it does not reveal the underlying laws of motion. Analogously: knowing how many times per week on average someone goes to the supermarket does not tell you whether they are at work today or who they are having dinner with tonight---the average is a reduced description that discards the very information needed for the latter questions. The underlying driver is still the \emph{laws of motion}; the probability distribution of entropy $P(S)$ or $P_{\infty}(S;\lambda)$ is only an \emph{approximate probabilistic description} of the consequences of those laws under the given coarse-graining and constraints. So to understand or exploit the system, one must study the motion and the constraints, not entropy itself.

\end{enumerate}

\vspace{1em}
\noindent
\textit{Prepared by Ting Peng (\href{mailto:t.peng@ieee.org}{t.peng@ieee.org}). For updates or further questions, contact the author.}

\end{document}
